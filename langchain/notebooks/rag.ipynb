{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/get_started/installation\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-15-preview\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"<api-key>\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"ada-embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install LangChain, you have two options: the official release or from source.\n",
      "\n",
      "1. Official release:\n",
      "   - To install LangChain, run the following command:\n",
      "     ```\n",
      "     pip install langchain\n",
      "     ```\n",
      "     or\n",
      "     ```\n",
      "     conda install langchain -c conda-forge\n",
      "     ```\n",
      "   - This will install the basic requirements of LangChain.\n",
      "\n",
      "2. From source:\n",
      "   - If you prefer to install from source, you can clone the LangChain repository and navigate to the `langchain` directory.\n",
      "   - Then run the following command:\n",
      "     ```\n",
      "     pip install -e .\n",
      "     ```\n",
      "\n",
      "It's important to note that while the basic requirements of LangChain will be installed, the dependencies needed for specific integrations (with model providers, datastores, etc.) are not included by default. You will need to install these dependencies separately.\n",
      "\n",
      "Additionally, there are other packages available within the LangChain ecosystem that you may find useful:\n",
      "\n",
      "- LangChain community: Contains third-party integrations. Install with:\n",
      "  ```\n",
      "  pip install langchain-community\n",
      "  ```\n",
      "\n",
      "- LangChain core: Contains base abstractions and the LangChain Expression Language. Install with:\n",
      "  ```\n",
      "  pip install langchain-core\n",
      "  ```\n",
      "\n",
      "- LangChain experimental: Holds experimental LangChain code for research and experimental purposes. Install with:\n",
      "  ```\n",
      "  pip install langchain-experimental\n",
      "  ```\n",
      "\n",
      "- LangServe: Helps deploy LangChain runnables and chains as a REST API. It is automatically installed by LangChain CLI. If not using LangChain CLI, install with:\n",
      "  ```\n",
      "  pip install \"langserve[all]\"\n",
      "  ```\n",
      "  or\n",
      "  ```\n",
      "  pip install \"langserve[client]\"    # for client code\n",
      "  pip install \"langserve[server]\"    # for server code\n",
      "  ```\n",
      "\n",
      "- LangChain CLI: Useful for working with LangChain templates and other LangServe projects. Install with:\n",
      "  ```\n",
      "  pip install langchain-cli\n",
      "  ```\n",
      "\n",
      "- LangSmith SDK: Automatically installed by LangChain. If not using LangChain, install with:\n",
      "  ```\n",
      "  pip install langsmith\n",
      "  ```\n",
      "\n",
      "Please note that the above information is based on the provided context and may be subject to change.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "context = [HumanMessage(content=\"Can I install langchain directly from source?\"), AIMessage(content=\"Yes!\")]\n",
    "\n",
    "query = \"Tell me how\"\n",
    "\n",
    "output = retrieval_chain.invoke({\n",
    "    \"context\": context,\n",
    "    \"input\": query\n",
    "})\n",
    "\n",
    "print(output[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
